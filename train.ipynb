{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fea8b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "import numpy as np\n",
    "from network import ChessNetV2 as ChessNet\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from replay_buffer import ReplayBuffer\n",
    "from state import move_to_index, move_mask\n",
    "import mcts\n",
    "import util\n",
    "import gc\n",
    "import time\n",
    "from device import device\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dc18d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(replay_buffer: ReplayBuffer, batch_size, net, optimizer):\n",
    "    net.train()\n",
    "    if replay_buffer.size() < batch_size:\n",
    "        return 0.0, 0.0, 0.0\n",
    "\n",
    "    boards, positions, target_policies, target_values = replay_buffer.sample(batch_size)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    pred_policies, pred_values = net(positions)\n",
    "\n",
    "    masks = torch.stack([torch.tensor(move_mask(board), dtype=torch.float32) for board in boards]).to(device)\n",
    "    masked_logits = pred_policies + (masks - 1) * 1e9\n",
    "    pred_probs = F.log_softmax(masked_logits, dim=1)\n",
    "\n",
    "    policy_loss = -torch.sum(target_policies * pred_probs, dim=1).mean()\n",
    "\n",
    "    value_loss = F.mse_loss(pred_values.squeeze(-1), target_values)\n",
    "    loss = policy_loss + value_loss\n",
    "\n",
    "    loss.backward()\n",
    "    \n",
    "    grad_norm = torch.nn.utils.clip_grad_norm_(net.parameters(), 5.0)\n",
    "    \n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item(), policy_loss.item(), value_loss.item(), grad_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "357796e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abhinavsrivatsa/projects/chess-bot/env/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint_path = 'model.pth'\n",
    "optimizer_checkpoint_path = 'optimizer.pth'\n",
    "replay_buffer_checkpoint_path = 'replay-buffer.pkl'\n",
    "game_gif_path = 'game.gif'\n",
    "\n",
    "net = ChessNet(n_moves=len(move_to_index))\n",
    "try:\n",
    "    net.load_state_dict(torch.load(model_checkpoint_path), device=device)\n",
    "except: # checkpoint doesn't exist, continue with new model\n",
    "    pass\n",
    "net = net.to(device)\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=5e-5, weight_decay=1e-5)\n",
    "try:\n",
    "    optimizer.load_state_dict(torch.load(optimizer_checkpoint_path))\n",
    "except: # checkpoint doesn't exist, continue with new optimizer\n",
    "    pass\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='min', \n",
    "    factor=0.7,\n",
    "    patience=3,\n",
    "    min_lr=1e-6,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "n_epochs = 50\n",
    "n_selfplay_games = 24\n",
    "n_train_steps = 1000\n",
    "\n",
    "replay_buffer = ReplayBuffer(500000, pct_recent=0.3, pct_recent_util=0.6)\n",
    "try:\n",
    "    replay_buffer.load(replay_buffer_checkpoint_path)\n",
    "except: # checkpoint doesn't exist, continue with new replay buffer\n",
    "    pass\n",
    "batch_size = 64\n",
    "\n",
    "n_sims = 800\n",
    "c_puct = 1.5\n",
    "temperature = 1.5\n",
    "temperature_threshold = 15\n",
    "alpha = 0.15\n",
    "epsilon = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7cc8a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: Starting selfplay...\n",
      "Added 5842 positions. Buffer: 5842\n",
      "Win Results - White: 0, Black: 0, Draw: 0\n",
      "\n",
      "Epoch 1: Loss: 2.7619, P_Loss: 2.5230, V_loss: 0.2389, Avg_GradNorm: 6.02, LR: 0.000050, Time: 56.25m\n",
      "New best loss! Saving checkpoint...\n",
      "\n",
      "Epoch 2: Starting selfplay...\n",
      "Added 8121 positions. Buffer: 13963\n",
      "Win Results - White: 0, Black: 0, Draw: 0\n",
      "\n",
      "Epoch 2: Loss: 2.1981, P_Loss: 1.9914, V_loss: 0.2067, Avg_GradNorm: 9.22, LR: 0.000050, Time: 104.04m\n",
      "New best loss! Saving checkpoint...\n",
      "\n",
      "Epoch 3: Starting selfplay...\n",
      "Added 8739 positions. Buffer: 22702\n",
      "Win Results - White: 0, Black: 0, Draw: 0\n",
      "\n",
      "Epoch 3: Loss: 2.0751, P_Loss: 1.8868, V_loss: 0.1883, Avg_GradNorm: 10.30, LR: 0.000050, Time: 126.74m\n",
      "New best loss! Saving checkpoint...\n",
      "\n",
      "Epoch 4: Starting selfplay...\n",
      "Added 7460 positions. Buffer: 30162\n",
      "Win Results - White: 0, Black: 0, Draw: 0\n",
      "\n",
      "Epoch 4: Loss: 2.0843, P_Loss: 1.8456, V_loss: 0.2387, Avg_GradNorm: 11.02, LR: 0.000050, Time: 197.00m\n",
      "No improvement. Patience: 1/15\n",
      "\n",
      "Epoch 5: Starting selfplay...\n",
      "Added 7151 positions. Buffer: 37313\n",
      "Win Results - White: 0, Black: 0, Draw: 0\n",
      "\n",
      "Epoch 5: Loss: 2.1956, P_Loss: 1.9423, V_loss: 0.2533, Avg_GradNorm: 11.32, LR: 0.000050, Time: 91.83m\n",
      "No improvement. Patience: 2/15\n",
      "\n",
      "Epoch 6: Starting selfplay...\n",
      "Added 6464 positions. Buffer: 43777\n",
      "Win Results - White: 0, Black: 0, Draw: 0\n",
      "\n",
      "Epoch 6: Loss: 2.2242, P_Loss: 1.9591, V_loss: 0.2651, Avg_GradNorm: 11.74, LR: 0.000050, Time: 93.09m\n",
      "No improvement. Patience: 3/15\n",
      "\n",
      "Epoch 7: Starting selfplay...\n",
      "Added 6123 positions. Buffer: 49900\n",
      "Win Results - White: 0, Black: 0, Draw: 0\n",
      "\n",
      "Epoch 7: Loss: 2.2794, P_Loss: 2.0090, V_loss: 0.2704, Avg_GradNorm: 12.00, LR: 0.000035, Time: 78.15m\n",
      "No improvement. Patience: 4/15\n",
      "Learning rate reduced: 0.000050 -> 0.000035\n",
      "\n",
      "Epoch 8: Starting selfplay...\n",
      "Added 6687 positions. Buffer: 56587\n",
      "Win Results - White: 0, Black: 0, Draw: 0\n",
      "\n",
      "Epoch 8: Loss: 2.2571, P_Loss: 2.0074, V_loss: 0.2497, Avg_GradNorm: 12.49, LR: 0.000035, Time: 88.05m\n",
      "No improvement. Patience: 5/15\n",
      "\n",
      "Epoch 9: Starting selfplay...\n",
      "Added 6785 positions. Buffer: 63372\n",
      "Win Results - White: 0, Black: 0, Draw: 0\n",
      "\n",
      "Epoch 9: Loss: 2.2258, P_Loss: 1.9850, V_loss: 0.2408, Avg_GradNorm: 13.38, LR: 0.000035, Time: 93.16m\n",
      "No improvement. Patience: 6/15\n",
      "\n",
      "Epoch 10: Starting selfplay...\n",
      "Added 7729 positions. Buffer: 71101\n",
      "Win Results - White: 0, Black: 0, Draw: 0\n",
      "\n",
      "Epoch 10: Loss: 2.2573, P_Loss: 2.0057, V_loss: 0.2516, Avg_GradNorm: 14.13, LR: 0.000035, Time: 173.86m\n",
      "No improvement. Patience: 7/15\n",
      "\n",
      "Epoch 11: Starting selfplay...\n",
      "Added 6942 positions. Buffer: 78043\n",
      "Win Results - White: 0, Black: 0, Draw: 0\n",
      "\n",
      "Epoch 11: Loss: 2.2420, P_Loss: 2.0079, V_loss: 0.2341, Avg_GradNorm: 13.91, LR: 0.000024, Time: 115.62m\n",
      "No improvement. Patience: 8/15\n",
      "Learning rate reduced: 0.000035 -> 0.000024\n",
      "\n",
      "Epoch 12: Starting selfplay...\n",
      "Added 7097 positions. Buffer: 85140\n",
      "Win Results - White: 0, Black: 0, Draw: 0\n",
      "\n",
      "Epoch 12: Loss: 2.2103, P_Loss: 1.9970, V_loss: 0.2133, Avg_GradNorm: 14.21, LR: 0.000024, Time: 112.27m\n",
      "No improvement. Patience: 9/15\n",
      "\n",
      "Epoch 13: Starting selfplay...\n",
      "Added 7416 positions. Buffer: 92556\n",
      "Win Results - White: 0, Black: 0, Draw: 0\n",
      "\n",
      "Epoch 13: Loss: 2.2403, P_Loss: 2.0177, V_loss: 0.2226, Avg_GradNorm: 15.15, LR: 0.000024, Time: 139.07m\n",
      "No improvement. Patience: 10/15\n",
      "\n",
      "Epoch 14: Starting selfplay...\n",
      "Added 7354 positions. Buffer: 99910\n",
      "Win Results - White: 0, Black: 0, Draw: 0\n",
      "\n",
      "Epoch 14: Loss: 2.2309, P_Loss: 2.0133, V_loss: 0.2177, Avg_GradNorm: 15.50, LR: 0.000024, Time: 137.25m\n",
      "No improvement. Patience: 11/15\n",
      "\n",
      "Epoch 15: Starting selfplay...\n",
      "Added 8620 positions. Buffer: 108530\n",
      "Win Results - White: 0, Black: 0, Draw: 0\n",
      "\n",
      "Epoch 15: Loss: 2.2473, P_Loss: 2.0311, V_loss: 0.2162, Avg_GradNorm: 15.90, LR: 0.000017, Time: 141.77m\n",
      "No improvement. Patience: 12/15\n",
      "Learning rate reduced: 0.000024 -> 0.000017\n",
      "\n",
      "Epoch 16: Starting selfplay...\n",
      "Added 7494 positions. Buffer: 116024\n",
      "Win Results - White: 0, Black: 0, Draw: 0\n",
      "\n",
      "Epoch 16: Loss: 2.2503, P_Loss: 2.0162, V_loss: 0.2341, Avg_GradNorm: 16.54, LR: 0.000017, Time: 192.19m\n",
      "No improvement. Patience: 13/15\n",
      "\n",
      "Epoch 17: Starting selfplay...\n",
      "Added 7048 positions. Buffer: 123072\n",
      "Win Results - White: 0, Black: 0, Draw: 0\n",
      "\n",
      "Epoch 17: Loss: 2.2581, P_Loss: 2.0270, V_loss: 0.2312, Avg_GradNorm: 17.25, LR: 0.000017, Time: 135.47m\n",
      "No improvement. Patience: 14/15\n",
      "\n",
      "Epoch 18: Starting selfplay...\n",
      "Added 6759 positions. Buffer: 129831\n",
      "Win Results - White: 0, Black: 0, Draw: 0\n",
      "\n",
      "Epoch 18: Loss: 2.2747, P_Loss: 2.0400, V_loss: 0.2347, Avg_GradNorm: 17.62, LR: 0.000017, Time: 170.11m\n",
      "No improvement. Patience: 15/15\n",
      "Early stopping triggered!\n"
     ]
    }
   ],
   "source": [
    "torch.save(net.state_dict(), model_checkpoint_path)\n",
    "torch.save(optimizer.state_dict(), optimizer_checkpoint_path)\n",
    "replay_buffer.save(replay_buffer_checkpoint_path)\n",
    "\n",
    "best_loss = float('inf')\n",
    "patience_counter = 0\n",
    "max_patience = 15\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_dir = os.path.join('games', f'epoch{epoch + 1}')\n",
    "    os.makedirs(epoch_dir, exist_ok=True)\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    print(f\"\\nEpoch {epoch + 1}: Starting selfplay...\")\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=12) as executor:\n",
    "        futures = [\n",
    "            executor.submit(\n",
    "                mcts.selfplay_wrapper, \n",
    "                model_checkpoint_path,\n",
    "                n_sims, \n",
    "                os.path.join(epoch_dir, f'game{i + 1}.txt'),\n",
    "                c_puct, \n",
    "                temperature,\n",
    "                temperature_threshold,\n",
    "                alpha, \n",
    "                epsilon,\n",
    "            ) for i in range(n_selfplay_games)\n",
    "        ]\n",
    "\n",
    "        positions_added = 0\n",
    "        game_results = {'1-0': 0, '0-1': 0, '1/2-1/2': 0}\n",
    "\n",
    "        for future in futures:\n",
    "            boards, positions, policies, values = future.result()\n",
    "            replay_buffer.add_game(boards, positions, policies, values)\n",
    "            positions_added += len(positions)\n",
    "\n",
    "            result = boards[-1].result()\n",
    "            if result in game_results:\n",
    "                game_results[result] += 1\n",
    "            \n",
    "    print(f\"Added {positions_added} positions. Buffer: {replay_buffer.size()}\")\n",
    "    print(f\"Win Results - White: {game_results.get('1-0', 0)}, \"\n",
    "          f\"Black: {game_results.get('0-1', 0)}, \"\n",
    "          f\"Draw: {game_results.get('1/2-1/2', 0)}\")\n",
    "    \n",
    "    total_loss = 0\n",
    "    total_ploss = 0\n",
    "    total_vloss = 0\n",
    "    total_grad_norm = 0\n",
    "    \n",
    "    for step in range(n_train_steps):\n",
    "        loss, ploss, vloss, grad_norm = train_step(replay_buffer, batch_size, net, optimizer)\n",
    "        total_loss += loss\n",
    "        total_ploss += ploss\n",
    "        total_vloss += vloss\n",
    "        total_grad_norm += grad_norm\n",
    "\n",
    "    \n",
    "    avg_loss = total_loss / n_train_steps\n",
    "    avg_ploss = total_ploss / n_train_steps\n",
    "    avg_vloss = total_vloss / n_train_steps\n",
    "    avg_grad_norm = total_grad_norm / n_train_steps\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    old_lr = optimizer.param_groups[0]['lr']\n",
    "    scheduler.step(avg_loss)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    print(f'\\nEpoch {epoch + 1}: Loss: {avg_loss:.4f}, P_Loss: {avg_ploss:.4f}, '\n",
    "          f'V_loss: {avg_vloss:.4f}, Avg_GradNorm: {avg_grad_norm:.2f}, '\n",
    "          f'LR: {current_lr:.6f}, Time: {(end - start)/60:.2f}m')\n",
    "    \n",
    "    if avg_loss < best_loss - 0.01:\n",
    "        best_loss = avg_loss\n",
    "        patience_counter = 0\n",
    "        print(f\"New best loss! Saving checkpoint...\")\n",
    "        torch.save(net.state_dict(), model_checkpoint_path)\n",
    "        torch.save(optimizer.state_dict(), optimizer_checkpoint_path)\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"No improvement. Patience: {patience_counter}/{max_patience}\")\n",
    "        if patience_counter >= max_patience:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "    \n",
    "    replay_buffer.save(replay_buffer_checkpoint_path)\n",
    "\n",
    "    if current_lr != old_lr:\n",
    "        print(f\"Learning rate reduced: {old_lr:.6f} -> {current_lr:.6f}\")\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece94c8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
