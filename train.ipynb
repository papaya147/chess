{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fea8b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "import numpy as np\n",
    "from network import ChessNetV2 as ChessNet\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from replay_buffer import ReplayBuffer\n",
    "from state import move_to_index, move_mask\n",
    "import mcts\n",
    "import util\n",
    "import gc\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de6c7112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS device\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print('Using MPS device')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('MPS not available, using CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dc18d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(replay_buffer: ReplayBuffer, batch_size, net, optimizer):\n",
    "    net.train()\n",
    "\n",
    "    if replay_buffer.size() < batch_size:\n",
    "        return 0.0, 0.0, 0.0\n",
    "\n",
    "    boards, positions, target_policies, target_values = replay_buffer.sample(batch_size)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    pred_policies, pred_values = net(positions)\n",
    "\n",
    "    masks = torch.stack([torch.tensor(move_mask(board), dtype=torch.float32) for board in boards]).to(device)\n",
    "    masked_policies = pred_policies * masks\n",
    "    target_sums = masked_policies.sum(dim=1, keepdim=True)\n",
    "    masked_policies = masked_policies / (target_sums + 1e-10)\n",
    "    \n",
    "    probs = F.log_softmax(masked_policies, dim=1)\n",
    "    policy_loss = -torch.sum(target_policies * probs) / batch_size\n",
    "\n",
    "    value_loss = F.mse_loss(pred_values.squeeze(-1), target_values)\n",
    "\n",
    "    loss = policy_loss + value_loss\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item(), policy_loss.item(), value_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "357796e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_path = 'model.pth'\n",
    "optimizer_checkpoint_path = 'optimizer.pth'\n",
    "replay_buffer_checkpoint_path = 'replay-buffer.pkl'\n",
    "game_gif_path = 'game.gif'\n",
    "\n",
    "net = ChessNet(n_moves=len(move_to_index))\n",
    "try:\n",
    "    net.load_state_dict(torch.load(model_checkpoint_path), device=device)\n",
    "except: # checkpoint doesn't exist, continue with new model\n",
    "    pass\n",
    "net = net.to(device)\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-3)\n",
    "try:\n",
    "    optimizer.load_state_dict(torch.load(optimizer_checkpoint_path))\n",
    "except: # checkpoint doesn't exist, continue with new optimizer\n",
    "    pass\n",
    "\n",
    "n_epochs = 50\n",
    "n_selfplay_games = 12\n",
    "\n",
    "replay_buffer = ReplayBuffer(1000000)\n",
    "try:\n",
    "    replay_buffer.load(replay_buffer_checkpoint_path)\n",
    "except: # checkpoint doesn't exist, continue with new replay buffer\n",
    "    pass\n",
    "batch_size = 64\n",
    "\n",
    "n_sims = 400\n",
    "c_puct = 1.5\n",
    "temperature = 1.5\n",
    "alpha = 0.1\n",
    "epsilon = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7cc8a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process SpawnProcess-11:\n",
      "Process SpawnProcess-3:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/abhinavsrivatsa/projects/chess/env/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/abhinavsrivatsa/projects/chess/env/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/abhinavsrivatsa/projects/chess/env/lib/python3.11/concurrent/futures/process.py\", line 249, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/abhinavsrivatsa/projects/chess/env/lib/python3.11/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/abhinavsrivatsa/projects/chess/env/lib/python3.11/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/abhinavsrivatsa/projects/chess/env/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/abhinavsrivatsa/projects/chess/env/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/abhinavsrivatsa/projects/chess/env/lib/python3.11/concurrent/futures/process.py\", line 249, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/abhinavsrivatsa/projects/chess/env/lib/python3.11/multiprocessing/queues.py\", line 103, in get\n",
      "    res = self._recv_bytes()\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/abhinavsrivatsa/projects/chess/env/lib/python3.11/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/abhinavsrivatsa/projects/chess/env/lib/python3.11/multiprocessing/connection.py\", line 430, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "          ^^^^^^^^^^^^^\n",
      "  File \"/Users/abhinavsrivatsa/projects/chess/env/lib/python3.11/multiprocessing/connection.py\", line 395, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m      9\u001b[39m     futures = [\n\u001b[32m     10\u001b[39m         executor.submit(\n\u001b[32m     11\u001b[39m             mcts.selfplay_wrapper, model_checkpoint_path, n_sims, c_puct, temperature, alpha, epsilon\n\u001b[32m     12\u001b[39m         ) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_selfplay_games)\n\u001b[32m     13\u001b[39m     ]\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m futures:\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m         boards, positions, policies, values = \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m         replay_buffer.add_game(boards, positions, policies, values)\n\u001b[32m     19\u001b[39m end = time.time()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/chess/env/lib/python3.11/concurrent/futures/_base.py:451\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m    449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__get_result()\n\u001b[32m--> \u001b[39m\u001b[32m451\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_condition\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/chess/env/lib/python3.11/threading.py:327\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m         \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    328\u001b[39m         gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    329\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "torch.save(net.state_dict(), model_checkpoint_path)\n",
    "torch.save(optimizer.state_dict(), optimizer_checkpoint_path)\n",
    "replay_buffer.save(replay_buffer_checkpoint_path)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=12) as executor:\n",
    "        futures = [\n",
    "            executor.submit(\n",
    "                mcts.selfplay_wrapper, model_checkpoint_path, n_sims, c_puct, temperature, alpha, epsilon\n",
    "            ) for _ in range(n_selfplay_games)\n",
    "        ]\n",
    "\n",
    "        for future in futures:\n",
    "            boards, positions, policies, values = future.result()\n",
    "            replay_buffer.add_game(boards, positions, policies, values)\n",
    "        \n",
    "    end = time.time()\n",
    "\n",
    "    # util.save_game_gif(board, game_gif_path)\n",
    "\n",
    "    loss, ploss, vloss = train_step(replay_buffer, batch_size, net, optimizer)\n",
    "    print(f'Epoch {epoch + 1}: Loss: {loss:.4f}, P_Loss: {ploss:.4f}, V_loss: {vloss:.4f}, Time: {(end - start):.2f}s')\n",
    "\n",
    "    torch.save(net.state_dict(), model_checkpoint_path)\n",
    "    torch.save(optimizer.state_dict(), optimizer_checkpoint_path)\n",
    "    replay_buffer.save(replay_buffer_checkpoint_path)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece94c8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
