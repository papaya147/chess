{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fea8b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "import numpy as np\n",
    "from network import ChessNetV2 as ChessNet\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from replay_buffer import ReplayBuffer\n",
    "from state import move_to_index, move_mask\n",
    "import mcts\n",
    "import util\n",
    "import gc\n",
    "import time\n",
    "from device import device\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dc18d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(replay_buffer: ReplayBuffer, batch_size, net, optimizer):\n",
    "    net.train()\n",
    "    if replay_buffer.size() < batch_size:\n",
    "        return 0.0, 0.0, 0.0\n",
    "\n",
    "    boards, positions, target_policies, target_values = replay_buffer.sample(batch_size)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    pred_policies, pred_values = net(positions)\n",
    "\n",
    "    masks = torch.stack([torch.tensor(move_mask(board), dtype=torch.float32) for board in boards]).to(device)\n",
    "    masked_logits = pred_policies + (masks - 1) * 1e9\n",
    "    pred_probs = F.log_softmax(masked_logits, dim=1)\n",
    "\n",
    "    policy_loss = -torch.sum(target_policies * pred_probs, dim=1).mean()\n",
    "\n",
    "    value_loss = F.mse_loss(pred_values.squeeze(-1), target_values)\n",
    "    loss = policy_loss + value_loss\n",
    "\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(net.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item(), policy_loss.item(), value_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357796e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_path = 'model.pth'\n",
    "optimizer_checkpoint_path = 'optimizer.pth'\n",
    "replay_buffer_checkpoint_path = 'replay-buffer.pkl'\n",
    "game_gif_path = 'game.gif'\n",
    "\n",
    "net = ChessNet(n_moves=len(move_to_index))\n",
    "try:\n",
    "    net.load_state_dict(torch.load(model_checkpoint_path), device=device)\n",
    "except: # checkpoint doesn't exist, continue with new model\n",
    "    pass\n",
    "net = net.to(device)\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=3e-4)\n",
    "try:\n",
    "    optimizer.load_state_dict(torch.load(optimizer_checkpoint_path))\n",
    "except: # checkpoint doesn't exist, continue with new optimizer\n",
    "    pass\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "n_epochs = 50\n",
    "n_selfplay_games = 12\n",
    "\n",
    "replay_buffer = ReplayBuffer(1000000)\n",
    "try:\n",
    "    replay_buffer.load(replay_buffer_checkpoint_path)\n",
    "except: # checkpoint doesn't exist, continue with new replay buffer\n",
    "    pass\n",
    "batch_size = 64\n",
    "\n",
    "n_sims = 400\n",
    "c_puct = 1.5\n",
    "temperature = 1.0\n",
    "temperature_threshold = 15\n",
    "alpha = 0.1\n",
    "epsilon = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cc8a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss: 2.2166, P_Loss: 2.1980, V_loss: 0.0186, Time: 1223.74s\n",
      "Epoch 2: Loss: 2.5781, P_Loss: 2.3176, V_loss: 0.2604, Time: 1469.88s\n",
      "Epoch 3: Loss: 2.2715, P_Loss: 2.2452, V_loss: 0.0263, Time: 2568.52s\n",
      "Epoch 4: Loss: 2.2144, P_Loss: 2.1946, V_loss: 0.0198, Time: 1139.67s\n",
      "Epoch 5: Loss: 2.2445, P_Loss: 2.2287, V_loss: 0.0159, Time: 1416.72s\n",
      "Epoch 6: Loss: 2.2849, P_Loss: 2.2745, V_loss: 0.0105, Time: 863.93s\n",
      "Epoch 7: Loss: 2.3242, P_Loss: 2.2403, V_loss: 0.0839, Time: 1241.08s\n",
      "Epoch 8: Loss: 2.2202, P_Loss: 2.2089, V_loss: 0.0113, Time: 1383.95s\n",
      "Epoch 9: Loss: 2.3462, P_Loss: 2.2463, V_loss: 0.0999, Time: 1488.04s\n",
      "Epoch 10: Loss: 2.2841, P_Loss: 2.2443, V_loss: 0.0397, Time: 1947.19s\n",
      "Epoch 11: Loss: 2.2534, P_Loss: 2.2426, V_loss: 0.0108, Time: 2963.55s\n",
      "Epoch 12: Loss: 2.2593, P_Loss: 2.2404, V_loss: 0.0189, Time: 1335.29s\n",
      "Epoch 13: Loss: 2.2448, P_Loss: 2.2355, V_loss: 0.0093, Time: 1327.39s\n",
      "Epoch 14: Loss: 2.2481, P_Loss: 2.2413, V_loss: 0.0068, Time: 2076.30s\n",
      "Epoch 15: Loss: 2.2423, P_Loss: 2.2163, V_loss: 0.0260, Time: 1306.99s\n",
      "Epoch 16: Loss: 2.3245, P_Loss: 2.2357, V_loss: 0.0888, Time: 2232.66s\n",
      "Epoch 17: Loss: 2.4088, P_Loss: 2.2488, V_loss: 0.1600, Time: 8008.03s\n",
      "Epoch 18: Loss: 2.2831, P_Loss: 2.2314, V_loss: 0.0517, Time: 9256.27s\n",
      "Epoch 19: Loss: 2.2811, P_Loss: 2.2491, V_loss: 0.0320, Time: 6734.66s\n",
      "Epoch 20: Loss: 2.3012, P_Loss: 2.2540, V_loss: 0.0471, Time: 3797.52s\n",
      "Epoch 21: Loss: 2.2432, P_Loss: 2.2218, V_loss: 0.0214, Time: 12080.57s\n",
      "Epoch 22: Loss: 2.2568, P_Loss: 2.2217, V_loss: 0.0351, Time: 1267.11s\n",
      "Epoch 23: Loss: 2.2752, P_Loss: 2.2488, V_loss: 0.0263, Time: 1846.66s\n",
      "Epoch 24: Loss: 2.2603, P_Loss: 2.2468, V_loss: 0.0135, Time: 1413.04s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process SpawnProcess-297:\n",
      "Process SpawnProcess-292:\n",
      "Process SpawnProcess-290:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/abhinavsrivatsa/projects/chess-bot/env/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/abhinavsrivatsa/projects/chess-bot/env/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/abhinavsrivatsa/projects/chess-bot/env/lib/python3.11/concurrent/futures/process.py\", line 249, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/abhinavsrivatsa/projects/chess-bot/env/lib/python3.11/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/abhinavsrivatsa/projects/chess-bot/env/lib/python3.11/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/abhinavsrivatsa/projects/chess-bot/env/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/abhinavsrivatsa/projects/chess-bot/env/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/abhinavsrivatsa/projects/chess-bot/env/lib/python3.11/concurrent/futures/process.py\", line 249, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/abhinavsrivatsa/projects/chess-bot/env/lib/python3.11/multiprocessing/queues.py\", line 103, in get\n",
      "    res = self._recv_bytes()\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/abhinavsrivatsa/projects/chess-bot/env/lib/python3.11/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/abhinavsrivatsa/projects/chess-bot/env/lib/python3.11/multiprocessing/connection.py\", line 430, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "          ^^^^^^^^^^^^^\n",
      "  File \"/Users/abhinavsrivatsa/projects/chess-bot/env/lib/python3.11/multiprocessing/connection.py\", line 395, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/abhinavsrivatsa/projects/chess-bot/env/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/abhinavsrivatsa/projects/chess-bot/env/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/abhinavsrivatsa/projects/chess-bot/env/lib/python3.11/concurrent/futures/process.py\", line 249, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/abhinavsrivatsa/projects/chess-bot/env/lib/python3.11/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/abhinavsrivatsa/projects/chess-bot/env/lib/python3.11/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     12\u001b[39m     futures = [\n\u001b[32m     13\u001b[39m         executor.submit(\n\u001b[32m     14\u001b[39m             mcts.selfplay_wrapper, \n\u001b[32m   (...)\u001b[39m\u001b[32m     22\u001b[39m         ) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_selfplay_games)\n\u001b[32m     23\u001b[39m     ]\n\u001b[32m     25\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m futures:\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m         boards, positions, policies, values = \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m         replay_buffer.add_game(boards, positions, policies, values)\n\u001b[32m     29\u001b[39m n_train_steps = \u001b[32m200\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/chess-bot/env/lib/python3.11/concurrent/futures/_base.py:451\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m    449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__get_result()\n\u001b[32m--> \u001b[39m\u001b[32m451\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_condition\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/chess-bot/env/lib/python3.11/threading.py:327\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m         \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    328\u001b[39m         gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    329\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "torch.save(net.state_dict(), model_checkpoint_path)\n",
    "torch.save(optimizer.state_dict(), optimizer_checkpoint_path)\n",
    "replay_buffer.save(replay_buffer_checkpoint_path)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_dir = os.path.join('games', f'epoch{epoch + 1}')\n",
    "    os.makedirs(epoch_dir, exist_ok=True)\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=12) as executor:\n",
    "        futures = [\n",
    "            executor.submit(\n",
    "                mcts.selfplay_wrapper, \n",
    "                model_checkpoint_path,\n",
    "                n_sims, \n",
    "                os.path.join(epoch_dir, f'game{i + 1}.txt'),\n",
    "                c_puct, \n",
    "                temperature,\n",
    "                temperature_threshold,\n",
    "                alpha, \n",
    "                epsilon,\n",
    "            ) for i in range(n_selfplay_games)\n",
    "        ]\n",
    "\n",
    "        for future in futures:\n",
    "            boards, positions, policies, values = future.result()\n",
    "            replay_buffer.add_game(boards, positions, policies, values)\n",
    "    \n",
    "    n_train_steps = 200\n",
    "    total_loss = 0\n",
    "    total_ploss = 0\n",
    "    total_vloss = 0\n",
    "    \n",
    "    for step in range(n_train_steps):\n",
    "        loss, ploss, vloss = train_step(replay_buffer, batch_size, net, optimizer)\n",
    "        total_loss += loss\n",
    "        total_ploss += ploss\n",
    "        total_vloss += vloss\n",
    "    \n",
    "    avg_loss = total_loss / n_train_steps\n",
    "    avg_ploss = total_ploss / n_train_steps\n",
    "    avg_vloss = total_vloss / n_train_steps\n",
    "\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    scheduler.step(avg_loss)\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}: Loss: {avg_loss:.4f}, P_Loss: {avg_ploss:.4f}, V_loss: {avg_vloss:.4f}, Time: {(end - start):.2f}s')\n",
    "\n",
    "    torch.save(net.state_dict(), model_checkpoint_path)\n",
    "    torch.save(optimizer.state_dict(), optimizer_checkpoint_path)\n",
    "    replay_buffer.save(replay_buffer_checkpoint_path)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece94c8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
