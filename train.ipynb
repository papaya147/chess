{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fea8b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "import numpy as np\n",
    "from network import ChessNetV2 as ChessNet\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from replay_buffer import ReplayBuffer\n",
    "from state import move_to_index, move_mask\n",
    "import mcts\n",
    "import util\n",
    "import gc\n",
    "import time\n",
    "from device import device\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dc18d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(replay_buffer: ReplayBuffer, batch_size, net, optimizer):\n",
    "    net.train()\n",
    "    if replay_buffer.size() < batch_size:\n",
    "        return 0.0, 0.0, 0.0\n",
    "\n",
    "    boards, positions, target_policies, target_values = replay_buffer.sample(batch_size)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    pred_policies, pred_values = net(positions)\n",
    "\n",
    "    masks = torch.stack([torch.tensor(move_mask(board), dtype=torch.float32) for board in boards]).to(device)\n",
    "    masked_logits = pred_policies + (masks - 1) * 1e9\n",
    "    pred_probs = F.log_softmax(masked_logits, dim=1)\n",
    "\n",
    "    policy_loss = -torch.sum(target_policies * pred_probs, dim=1).mean()\n",
    "\n",
    "    value_loss = F.mse_loss(pred_values.squeeze(-1), target_values)\n",
    "    loss = policy_loss + value_loss\n",
    "\n",
    "    loss.backward()\n",
    "    \n",
    "    grad_norm = torch.nn.utils.clip_grad_norm_(net.parameters(), 5.0)\n",
    "    \n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item(), policy_loss.item(), value_loss.item(), grad_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "357796e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abhinavsrivatsa/projects/chess-bot/env/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint_path = 'model.pth'\n",
    "optimizer_checkpoint_path = 'optimizer.pth'\n",
    "replay_buffer_checkpoint_path = 'replay-buffer.pkl'\n",
    "game_gif_path = 'game.gif'\n",
    "\n",
    "net = ChessNet(n_moves=len(move_to_index))\n",
    "try:\n",
    "    net.load_state_dict(torch.load(model_checkpoint_path), device=device)\n",
    "except: # checkpoint doesn't exist, continue with new model\n",
    "    pass\n",
    "net = net.to(device)\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=5e-5, weight_decay=1e-5)\n",
    "try:\n",
    "    optimizer.load_state_dict(torch.load(optimizer_checkpoint_path))\n",
    "except: # checkpoint doesn't exist, continue with new optimizer\n",
    "    pass\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='min', \n",
    "    factor=0.7,\n",
    "    patience=3,\n",
    "    min_lr=1e-6,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "n_epochs = 50\n",
    "n_selfplay_games = 24\n",
    "n_train_steps = 1000\n",
    "\n",
    "replay_buffer = ReplayBuffer(500000, pct_recent=0.3, pct_recent_util=0.6)\n",
    "try:\n",
    "    replay_buffer.load(replay_buffer_checkpoint_path)\n",
    "except: # checkpoint doesn't exist, continue with new replay buffer\n",
    "    pass\n",
    "batch_size = 64\n",
    "\n",
    "n_sims = 800\n",
    "c_puct = 1.5\n",
    "temperature = 1.5\n",
    "temperature_threshold = 15\n",
    "alpha = 0.15\n",
    "epsilon = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cc8a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: Starting selfplay...\n",
      "Added 6266 positions. Buffer: 6266\n",
      "Win Results - White: 0, Black: 0, Draw: 0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ReduceLROnPlateau.step() missing 1 required positional argument: 'metrics'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 70\u001b[39m\n\u001b[32m     67\u001b[39m end = time.time()\n\u001b[32m     69\u001b[39m old_lr = optimizer.param_groups[\u001b[32m0\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m \u001b[43mscheduler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m current_lr = optimizer.param_groups[\u001b[32m0\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     73\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, P_Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_ploss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     74\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mV_loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_vloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Avg_GradNorm: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_grad_norm\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     75\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mLR: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_lr\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(end\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39mstart)/\u001b[32m60\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mm\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: ReduceLROnPlateau.step() missing 1 required positional argument: 'metrics'"
     ]
    }
   ],
   "source": [
    "torch.save(net.state_dict(), model_checkpoint_path)\n",
    "torch.save(optimizer.state_dict(), optimizer_checkpoint_path)\n",
    "replay_buffer.save(replay_buffer_checkpoint_path)\n",
    "\n",
    "best_loss = float('inf')\n",
    "patience_counter = 0\n",
    "max_patience = 15\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_dir = os.path.join('games', f'epoch{epoch + 1}')\n",
    "    os.makedirs(epoch_dir, exist_ok=True)\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    print(f\"\\nEpoch {epoch + 1}: Starting selfplay...\")\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=12) as executor:\n",
    "        futures = [\n",
    "            executor.submit(\n",
    "                mcts.selfplay_wrapper, \n",
    "                model_checkpoint_path,\n",
    "                n_sims, \n",
    "                os.path.join(epoch_dir, f'game{i + 1}.txt'),\n",
    "                c_puct, \n",
    "                temperature,\n",
    "                temperature_threshold,\n",
    "                alpha, \n",
    "                epsilon,\n",
    "            ) for i in range(n_selfplay_games)\n",
    "        ]\n",
    "\n",
    "        positions_added = 0\n",
    "        game_results = {'1-0': 0, '0-1': 0, '1/2-1/2': 0}\n",
    "\n",
    "        for future in futures:\n",
    "            boards, positions, policies, values = future.result()\n",
    "            replay_buffer.add_game(boards, positions, policies, values)\n",
    "            positions_added += len(positions)\n",
    "\n",
    "            result = boards[-1].result()\n",
    "            if result in game_results:\n",
    "                game_results[result] += 1\n",
    "            \n",
    "    print(f\"Added {positions_added} positions. Buffer: {replay_buffer.size()}\")\n",
    "    print(f\"Win Results - White: {game_results.get('1-0', 0)}, \"\n",
    "          f\"Black: {game_results.get('0-1', 0)}, \"\n",
    "          f\"Draw: {game_results.get('1/2-1/2', 0)}\")\n",
    "    \n",
    "    total_loss = 0\n",
    "    total_ploss = 0\n",
    "    total_vloss = 0\n",
    "    total_grad_norm = 0\n",
    "    \n",
    "    for step in range(n_train_steps):\n",
    "        loss, ploss, vloss, grad_norm = train_step(replay_buffer, batch_size, net, optimizer)\n",
    "        total_loss += loss\n",
    "        total_ploss += ploss\n",
    "        total_vloss += vloss\n",
    "        total_grad_norm += grad_norm\n",
    "\n",
    "    \n",
    "    avg_loss = total_loss / n_train_steps\n",
    "    avg_ploss = total_ploss / n_train_steps\n",
    "    avg_vloss = total_vloss / n_train_steps\n",
    "    avg_grad_norm = total_grad_norm / n_train_steps\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    old_lr = optimizer.param_groups[0]['lr']\n",
    "    scheduler.step(avg_loss)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    print(f'\\nEpoch {epoch + 1}: Loss: {avg_loss:.4f}, P_Loss: {avg_ploss:.4f}, '\n",
    "          f'V_loss: {avg_vloss:.4f}, Avg_GradNorm: {avg_grad_norm:.2f}, '\n",
    "          f'LR: {current_lr:.6f}, Time: {(end - start)/60:.2f}m')\n",
    "    \n",
    "    if avg_loss < best_loss - 0.01:\n",
    "        best_loss = avg_loss\n",
    "        patience_counter = 0\n",
    "        print(f\"New best loss! Saving checkpoint...\")\n",
    "        torch.save(net.state_dict(), model_checkpoint_path)\n",
    "        torch.save(optimizer.state_dict(), optimizer_checkpoint_path)\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"No improvement. Patience: {patience_counter}/{max_patience}\")\n",
    "        if patience_counter >= max_patience:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "    \n",
    "    replay_buffer.save(replay_buffer_checkpoint_path)\n",
    "\n",
    "    if current_lr != old_lr:\n",
    "        print(f\"Learning rate reduced: {old_lr:.6f} -> {current_lr:.6f}\")\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece94c8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
