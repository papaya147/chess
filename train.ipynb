{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fea8b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "import numpy as np\n",
    "from network import ChessNetV2 as ChessNet\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from replay_buffer import ReplayBuffer\n",
    "from state import move_to_index, move_mask\n",
    "import mcts\n",
    "import util\n",
    "import gc\n",
    "import time\n",
    "from device import device\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dc18d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(replay_buffer: ReplayBuffer, batch_size, net, optimizer, scheduler):\n",
    "    net.train()\n",
    "\n",
    "    if replay_buffer.size() < batch_size:\n",
    "        return 0.0, 0.0, 0.0\n",
    "\n",
    "    boards, positions, target_policies, target_values = replay_buffer.sample(batch_size)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    pred_policies, pred_values = net(positions)\n",
    "\n",
    "    masks = torch.stack([torch.tensor(move_mask(board), dtype=torch.float32) for board in boards]).to(device)\n",
    "    masked_logits = pred_policies + (masks - 1) * 10\n",
    "    policy_loss = F.cross_entropy(masked_logits, target_policies)\n",
    "\n",
    "    value_loss = F.mse_loss(pred_values.squeeze(-1), target_values)\n",
    "\n",
    "    loss = policy_loss + 0.5 * value_loss\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step(loss)\n",
    "\n",
    "    return loss.item(), policy_loss.item(), value_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "357796e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_path = 'model.pth'\n",
    "optimizer_checkpoint_path = 'optimizer.pth'\n",
    "replay_buffer_checkpoint_path = 'replay-buffer.pkl'\n",
    "game_gif_path = 'game.gif'\n",
    "\n",
    "net = ChessNet(n_moves=len(move_to_index))\n",
    "try:\n",
    "    net.load_state_dict(torch.load(model_checkpoint_path), device=device)\n",
    "except: # checkpoint doesn't exist, continue with new model\n",
    "    pass\n",
    "net = net.to(device)\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-3)\n",
    "try:\n",
    "    optimizer.load_state_dict(torch.load(optimizer_checkpoint_path))\n",
    "except: # checkpoint doesn't exist, continue with new optimizer\n",
    "    pass\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "n_epochs = 50\n",
    "n_selfplay_games = 12\n",
    "\n",
    "replay_buffer = ReplayBuffer(1000000)\n",
    "try:\n",
    "    replay_buffer.load(replay_buffer_checkpoint_path)\n",
    "except: # checkpoint doesn't exist, continue with new replay buffer\n",
    "    pass\n",
    "batch_size = 64\n",
    "\n",
    "n_sims = 800\n",
    "c_puct = 1.5\n",
    "temperature = 1.5\n",
    "alpha = 0.1\n",
    "epsilon = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cc8a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), model_checkpoint_path)\n",
    "torch.save(optimizer.state_dict(), optimizer_checkpoint_path)\n",
    "replay_buffer.save(replay_buffer_checkpoint_path)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_dir = os.path.join('games', f'epoch{epoch + 1}')\n",
    "    try:\n",
    "        os.mkdir(epoch_dir)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=12) as executor:\n",
    "        futures = [\n",
    "            executor.submit(\n",
    "                mcts.selfplay_wrapper, \n",
    "                model_checkpoint_path,\n",
    "                n_sims, \n",
    "                os.path.join(epoch_dir, f'game{i + 1}.txt'),\n",
    "                c_puct, \n",
    "                temperature, \n",
    "                alpha, \n",
    "                epsilon,\n",
    "            ) for i in range(n_selfplay_games)\n",
    "        ]\n",
    "\n",
    "        for future in futures:\n",
    "            boards, positions, policies, values = future.result()\n",
    "            replay_buffer.add_game(boards, positions, policies, values)\n",
    "        \n",
    "    end = time.time()\n",
    "\n",
    "    # util.save_game_gif(board, game_gif_path)\n",
    "\n",
    "    loss, ploss, vloss = train_step(replay_buffer, batch_size, net, optimizer, scheduler)\n",
    "    print(f'Epoch {epoch + 1}: Loss: {loss:.4f}, P_Loss: {ploss:.4f}, V_loss: {vloss:.4f}, Time: {(end - start):.2f}s')\n",
    "\n",
    "    torch.save(net.state_dict(), model_checkpoint_path)\n",
    "    torch.save(optimizer.state_dict(), optimizer_checkpoint_path)\n",
    "    replay_buffer.save(replay_buffer_checkpoint_path)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece94c8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
